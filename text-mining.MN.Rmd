---
title: "Text Analyis"
output: github_document
---

# Your mission

Perform text analysis.

## Okay, I need more information

Perform sentiment analysis or topic modeling using text analysis methods as demonstrated in the pre-class work and in the readings.

## Okay, I need even more information.

Do the above. Can't think of a data source?

- `gutenbergr`
- `AssociatedPress` from the `topicmodels` package
- `NYTimes` or `USCongress` from the `RTextTools` package
- Harry Potter Complete 7 Books text

```{r eval=FALSE}
if (packageVersion("devtools") < 1.6) {
  install.packages("devtools")
}

devtools::install_github("bradleyboehmke/harrypotter")
```

- [State of the Union speeches](https://pradeepadhokshaja.wordpress.com/2017/03/31/scraping-the-web-for-presdential-inaugural-addresses-using-rvest/)

- Scrape tweets using [`twitteR`](https://www.r-bloggers.com/setting-up-the-twitter-r-package-for-text-analytics/)

- [Previous URL](https://www.credera.com/blog/business-intelligence/twitter-analytics-using-r-part-1-extract-tweets/)

Analyze the text for sentiment OR topic. **You do not need to do both**.

The datacamp courses and [Tidy Text Mining with R](http://tidytextmining.com/) are good starting points for templates to perform this type of analysis, but feel free to *expand beyond these examples*.

# Timelines and Task

We will spend the next 2 weeks working on analyzing textual data in R. You will do the following:

- Start with some text based data.
- Clean data and prepare it for analysis
- Ask questions about the data
- Answer these questions with the data using tables and graphics
- Each group member must have their own unique question that they code the answer for. 

##Twitter Extraction Libraries
```{r}
library(twitteR)
library(ROAuth)
library(tidyverse)
library(tidytext)
library(tm)
library(lubridate)
library(stringr)
library(sentimentr)
library(stm)
library(knitr)

```

Check out this website (http://politicaldatascience.blogspot.com/2015/12/rtutorial-using-r-to-harvest-twitter.html)

```{r echo=FALSE}
consumer_key <- "61QAqhSHk4n1wIGkkSJd4uWYc"
consumer_secret <- "Wp5fauhEPj66F3IsTJVeAznJrq6ozNBRhrfyIweMjiLhsV4IV0"
access_token <- "903307271752605696-3YvAs5p7R7y8ASXXLCQWRUnW0veUNFB"
access_secret <- "PyrBLoT3ScECCfOsP2MlzOIW8MQW11lLsnihXu8r8k24q"

setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
```
```{r, eval=FALSE}
guinness <- userTimeline("GuinnessUS", n=2000, since="2014-01-01")

guinness.td <- twListToDF(guinness)
write_csv(guinness.td, "guinness.csv")
```

From the collected tweets, it seems that what we need is (text, replyTOSN, created). 
```{r, eval=FALSE}
tweets.edit <- guinnes.td %>% 
  select(text, replyToSN, created) %>% 
  group_by(created) 

# fix date 
tweets.edit$month <- month(tweets.edit$created)
tweets.edit$day <- day(tweets.edit$created)
tweets.edit$year <- year(tweets.edit$created)
```


Almost done at this point, all we need to do is the sentiment analysis, though I perfer to use sentimentR as opposed to what we did in the datacamp courses because it leverages valence shifters, and views the entire tweet as a whole. But I'll do both...

```{r, eval=FALSE}
#take out text, tidy it, word per row, and run sentiment analysis via inner join
tweets <- tweets.edit$text
tweets.td <- tidy(tweets)

tweet.words <- tweets.td %>%
  unnest_tokens(word, x) %>% 
  anti_join(stop_words)

tweet_sentiment <- tweet.words %>%
  inner_join(get_sentiments("bing"), by = "word") %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()


tweets.edit.sentiment <- sentiment_by(tweets.edit$text)
```


After playing around, we decided it was best to combine all the raw data and then begin cleaning it. 
```{r}
clean.beer <- read_csv("AllCleanedTweets.csv")

#just want specific data as meta data --> data that would serve as a covariate: beer company and date 

beer.data <- clean.beer %>% 
  select(X1, text, screenName, weekday)

#process the data via STM textProcessor 
proc.beer <- textProcessor(documents = beer.data$text, metadata = beer.data)

out <- prepDocuments(proc.beer$documents, proc.beer$vocab, proc.beer$meta)
beerdocs <- out$documents 
beervocab <- out$vocab
beermeta <- out$meta
```

```{r, eval=F}
#need to instal Rtsne, rsvd, and geometry
  #install.packages("Rtsne")
  #install.packages("rsvd")
  #install.packages("geometry")

#from here, we can run the stm function for topic modelling 

set.seed(123)
beerModelFit <- stm(document = docs,
                    vocab = vocab,
                    K =  30, #we can set k to pretty much anything, used seearchK to find reasonable value
                    prevalence =~ screenName + weekday,
                    data = meta,
                    init.type = "Spectral",
                    ) 


#K determines the number of topics, we can use searchK to to figure out the appropriate number of topics
set.seed(5679305)
K <- c(5,10,15,20)
storage <- searchK(docs,
                   vocab,
                   K,
                   prevalence=~ screenName + weekday,
                   data = meta)

#reasonable K set at 30, model converaged after 423 iterations, saving model to be used for later 
saveRDS(beerModelFit, "beerModel.rds")

```

The STM package leverages metadata as part of the topic modelling process. This is denoted by the prevalence argument. It takes in the metadata: sceenName (the beer company) & weekday, and provides us with 30 topics for which the tweets can fall under. The two metadata is describing how the prevalence of the topics is influenced by the two covariates -- who is tweeting, and on what day are they tweeting. 

The first reason why screenName is an important covariate is quite simple: who is tweeting matters. We also have reason to believe that the topic of their tweets will change based on the day, ex. TGIF, or Monday night Football. Tess, will be the one who will help explore topic prevalance given day of the week. 

For now, I will focus on topic prevalence given the beer company. Who is tweeting, and what are they tweeting about? However, I am still using the weekday covariate in order to train my model to be as accurate as possible. Just because I will not focus on weekday does not mean that it is not necessary when developing our model. STM's greatest advantage over other topic modelling techniques is its ability to include metadata to help better describe and train the topic outputs. 

In order to investigate the beer companies, we need to first answer some more questions: 
- "What are the most popular topics?"
- "Who tweets about these topics the most?"
- "Which topics recieve the most engagement? (Favorites & Retweets)"

Thus we need to identify the topics with the highest frequenct among our tweets, visualize who is tweeting them, and then organize them engagement. 
```{r, include=T}
BeerModel <- readRDS("beerModel.rds")

BeerDocs <- beermeta$text

thoughts <- findThoughts(BeerModel, BeerDocs, topics=c(1,2,3), n=5)

plot(thoughts)
```